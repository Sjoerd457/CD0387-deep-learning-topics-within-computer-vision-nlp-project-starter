{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sjoerd457/CD0387-deep-learning-topics-within-computer-vision-nlp-project-starter/blob/train/train_and_deploy_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QhwIfzUCWlK"
      },
      "source": [
        "# TODO: Title\n",
        "\n",
        "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
        "\n",
        "\n",
        "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
        "\n",
        "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkxlaGcuCWlM",
        "outputId": "f0c56e0c-99f5-449a-b406-04331395ecc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting smdebug\n",
            "  Downloading smdebug-1.0.12-py2.py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from smdebug) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from smdebug) (3.17.3)\n",
            "Collecting pyinstrument==3.4.2\n",
            "  Downloading pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 833 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from smdebug) (21.3)\n",
            "Collecting boto3>=1.10.32\n",
            "  Downloading boto3-1.24.26-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 62.0 MB/s \n",
            "\u001b[?25hCollecting pyinstrument-cext>=0.2.2\n",
            "  Downloading pyinstrument_cext-0.2.4-cp37-cp37m-manylinux2010_x86_64.whl (20 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.26\n",
            "  Downloading botocore-1.27.26-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.26->boto3>=1.10.32->smdebug) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.0->smdebug) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->smdebug) (3.0.9)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, pyinstrument-cext, pyinstrument, boto3, smdebug\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.10 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.24.26 botocore-1.27.26 jmespath-1.0.1 pyinstrument-3.4.2 pyinstrument-cext-0.2.4 s3transfer-0.6.0 smdebug-1.0.12 urllib3-1.26.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0+cu113)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 7.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.10\n",
            "    Uninstalling urllib3-1.26.10:\n",
            "      Successfully uninstalled urllib3-1.26.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "# TODO: Install any packages that you might need\n",
        "# For instance, you will need the smdebug package\n",
        "!pip install smdebug\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AS-qEv4UCWlO"
      },
      "outputs": [],
      "source": [
        "# TODO: Import any packages that you might need\n",
        "# For instance you will need Boto3 and Sagemaker\n",
        "# import sagemaker\n",
        "# import boto3\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models # add models to the list\n",
        "from torchvision.utils import make_grid\n",
        "import os\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su9BNZHfCWlO"
      },
      "source": [
        "Start with initial data analysis: what do the images look like? How large are the images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klzOB56QCWlO"
      },
      "source": [
        "## Dataset\n",
        "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gfTQBYUhCWlP"
      },
      "outputs": [],
      "source": [
        "#TODO: Fetch and upload the data to AWS S3\n",
        "\n",
        "# Command to download and unzip data\n",
        "# !wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
        "# !unzip dogImages.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "23jLx3sRCWlP",
        "outputId": "a1138588-b00b-414b-adc4-b5bb8643f2b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-03df5387f2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'"
          ]
        }
      ],
      "source": [
        "test_path = \"./dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg\"\n",
        "\n",
        "with Image.open(test_path) as im:\n",
        "    display(im)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttOVk0gaCWlQ"
      },
      "outputs": [],
      "source": [
        "class DataInspectTools():\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "\n",
        "    def get_image_names(self):\n",
        "        '''Returns the names of the images in all subfolders within path'''\n",
        "        img_names = []\n",
        "        for folder, subfolders, filenames in os.walk(path):\n",
        "            for img in filenames:\n",
        "                img_names.append(folder+'/'+img)\n",
        "        return img_names\n",
        "\n",
        "   \n",
        "    def display_image_sizes(self):\n",
        "        '''Display statistics of image sizes in data'''\n",
        "        img_names = self.get_image_names()\n",
        "        img_sizes = []\n",
        "        rejected = []\n",
        "\n",
        "        for item in img_names:\n",
        "            try:\n",
        "                with Image.open(item) as img:\n",
        "                    img_sizes.append(img.size)\n",
        "            except:\n",
        "                rejected.append(item)\n",
        "                \n",
        "        print(f'Images:  {len(img_sizes)}')\n",
        "        print(f'Rejects: {len(rejected)}')\n",
        "        df = pd.DataFrame(img_sizes, columns=['width','length'])\n",
        "        display(df.describe())\n",
        "\n",
        "\n",
        "    def set_transforms(self):\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
        "            transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
        "            transforms.Resize(224),             # resize shortest side to 224 pixels\n",
        "            transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.test_transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "path = './dogImages/'\n",
        "d = DataInspectTools(path)\n",
        "d.display_image_sizes()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwLFhUNECWlQ"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
        "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
        "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
        "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w0y7yHuCWlR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, path, transform):\n",
        "        self.root = path\n",
        "        self.transform = transform\n",
        "        self.img_paths, self.img_class, self.img_class_int = self._get_image(path)\n",
        "        self.img_class_int = self._get_class_indexes()\n",
        "    \n",
        "    def _get_class_indexes(self):\n",
        "        '''Convert the classes to integers'''\n",
        "        self.unique_classes = list(np.unique(self.img_class))\n",
        "        return [self.unique_classes.index(c) for c in self.img_class]\n",
        "        \n",
        "    \n",
        "\n",
        "    def _get_image(self,path):\n",
        "        '''Returns the names of the images in all subfolders within path'''\n",
        "        img_loc = []\n",
        "        img_class = []\n",
        "        img_class_int = []\n",
        "        i = -1 \n",
        "        for folder, subfolders, filenames in os.walk(path):\n",
        "            for img in filenames:\n",
        "                img_loc.append(folder+'/'+img)\n",
        "                img_class.append(folder.split(\"/\")[-1])\n",
        "                img_class_int.append(i)\n",
        "            i+=1\n",
        "        return img_loc, img_class, img_class_int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.img_paths[idx])\n",
        "        target_class = self.img_class_int[idx]\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image, target_class\n",
        "\n",
        "    def _plot_tensor(self, img):\n",
        "        plt.imshow(np.transpose(img.numpy(), (1, 2, 0)));\n",
        "\n",
        "    def test_transform(self,idx = 0):\n",
        "        img_loc = os.path.join(self.img_paths[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        print(f'Image before transform (size {image.size})')\n",
        "        display(image)\n",
        "        tensor_image = self.transform(image)\n",
        "        print(f'Image after transform (size {tensor_image.size()})')\n",
        "        self._plot_tensor(tensor_image)\n",
        "\n",
        "    def get_class_names(self):\n",
        "        return list(np.unique(self.img_class))\n",
        "\n",
        "    def get_labels(self):\n",
        "        return list(np.unique(self.img_class_int))\n",
        "\n",
        "\n",
        "\n",
        "data = CustomDataSet(\"./dogImages/test/\", train_transform)\n",
        "\n",
        "data.test_transform(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjb-WsgoCWlR"
      },
      "outputs": [],
      "source": [
        "root = './dogImages/'\n",
        "seed = 41 #VO\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "train_data = CustomDataSet(os.path.join(root, 'train'), transform=train_transform)\n",
        "valid_data = CustomDataSet(os.path.join(root, 'valid'), transform=test_transform)\n",
        "test_data = CustomDataSet(os.path.join(root, 'test'), transform=test_transform)\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    train_loader = DataLoader(train_data, batch_size=10, shuffle=True, pin_memory=True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=10, shuffle=True, pin_memory=True)\n",
        "    test_data = DataLoader(valid_data, batch_size=10, shuffle=True, pin_memory=True)\n",
        "else:\n",
        "    train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_data, batch_size=10, shuffle=True)\n",
        "    test_data = DataLoader(valid_data, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[0]"
      ],
      "metadata": {
        "id": "hGD3prH6FGdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HduFq9gkCWlS"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(seed)\n",
        "\n",
        "class_names = train_data.unique_classes\n",
        "\n",
        "\n",
        "# Grab the first batch of 10 images\n",
        "for images,labels in train_loader: \n",
        "    break\n",
        "\n",
        "# Print the labelstrain\n",
        "print('Label:', labels)\n",
        "print('Class:', *np.array([class_names[i] for i in labels]))\n",
        "\n",
        "im = make_grid(images, nrow=5)  # the default nrow is 8\n",
        "\n",
        "# Inverse normalize the images\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")\n",
        "im_inv = inv_normalize(im)\n",
        "\n",
        "# Print the images\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRzKbfj8CWlS"
      },
      "outputs": [],
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
        "        self.fc1 = nn.Linear(54*54*16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = F.relu(self.conv2(X))\n",
        "        X = F.max_pool2d(X, 2, 2)\n",
        "        X = X.view(-1, 54*54*16)\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = self.fc3(X)\n",
        "        return F.log_softmax(X, dim=1)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        '''Returns the number of parameters to estimate in each layer'''\n",
        "        params = [p.numel() for p in self.parameters() if p.requires_grad]\n",
        "        for item in params:\n",
        "            print(f'{item:>8}')\n",
        "        print(f'________\\n{sum(params):>8}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RFoA54ZCWlS"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(seed)\n",
        "CNNmodel = ConvolutionalNetwork(133)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=0.001)\n",
        "CNNmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQSPyqXZCWlS"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "class ModelTrainer():\n",
        "    \n",
        "    def __init__(self, \n",
        "                 model,\n",
        "                 criterion,\n",
        "                 optimizer,\n",
        "                 epochs,\n",
        "                 max_trn_batch,\n",
        "                 max_tst_batch,\n",
        "                 train_loader,\n",
        "                 valid_loader            \n",
        "                 \n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "\n",
        "        #To limit the number of batches\n",
        "        self.max_trn_batch = max_trn_batch\n",
        "        self.max_tst_batch = max_tst_batch\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.train_correct = []\n",
        "        self.test_correct = []\n",
        "\n",
        "        self.run_model()\n",
        "\n",
        "\n",
        "    def run_model(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            self.train_model(epoch)\n",
        "            self.test_model()\n",
        "            \n",
        "    def _to_cuda(self, X, Y):\n",
        "        if(torch.cuda.is_available()):\n",
        "            X = X.cuda()\n",
        "            Y = Y.cuda()\n",
        "        return X, Y\n",
        "\n",
        "    def train_model(self, epoch):\n",
        "        trn_corr = 0\n",
        "        # Run the training batches\n",
        "        for b, (X_train, y_train) in enumerate(self.train_loader):\n",
        "            \n",
        "            X_train, y_train = self._to_cuda(X_train, y_train)\n",
        "            \n",
        "\n",
        "            \n",
        "            # Limit the number of batches\n",
        "            if b == max_trn_batch:\n",
        "                break\n",
        "            b+=1\n",
        "            \n",
        "            # Apply the model\n",
        "            y_pred = self.model(X_train)\n",
        "            loss = self.criterion(y_pred, y_train)\n",
        "    \n",
        "            # Tally the number of correct predictions\n",
        "            predicted = torch.max(y_pred.data, 1)[1]\n",
        "            batch_corr = (predicted == y_train).sum()\n",
        "            trn_corr += batch_corr\n",
        "            \n",
        "            # Update parameters\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Print interim results\n",
        "            if b%200 == 0:\n",
        "                print(f'epoch: {epoch:2}  batch: {b:4} [{10*b:6}/8000]  loss: {loss.item():10.8f}  \\\n",
        "                accuracy: {trn_corr.item()*100/(10*b):7.3f}%')  \n",
        "        \n",
        "        self.train_losses.append(loss)\n",
        "        self.train_correct.append(trn_corr)      \n",
        "\n",
        "    def test_model(self):\n",
        "        tst_corr = 0\n",
        "\n",
        "        # Run the testing batches\n",
        "        with torch.no_grad():\n",
        "            for b, (X_test, y_test) in enumerate(self.valid_loader):\n",
        "                \n",
        "                X_test, y_test = self._to_cuda(X_test, y_test)\n",
        "                \n",
        "                # Limit the number of batches\n",
        "                if b == self.max_tst_batch:\n",
        "                    break\n",
        "\n",
        "                # Apply the model\n",
        "                y_val = self.model(X_test)\n",
        "\n",
        "                # Tally the number of correct predictions\n",
        "                predicted = torch.max(y_val.data, 1)[1] \n",
        "                tst_corr += (predicted == y_test).sum()\n",
        "\n",
        "        loss = self.criterion(y_val, y_test)\n",
        "        self.test_losses.append(loss)\n",
        "        self.test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "\n",
        "CNNmodel = ConvolutionalNetwork(133)\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    CNNmodel = CNNmodel.cuda()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=0.01)\n",
        "epochs = 20\n",
        "max_trn_batch = 10000\n",
        "max_tst_batch = 10000\n",
        "\n",
        "type(CNNmodel)\n",
        "next(CNNmodel.parameters()).is_cuda\n",
        "trained_model = ModelTrainer(\n",
        "                 model=CNNmodel,\n",
        "                 criterion=criterion,\n",
        "                 optimizer=optimizer,\n",
        "                 epochs=epochs,\n",
        "                 max_trn_batch=max_trn_batch,\n",
        "                 max_tst_batch=max_tst_batch,\n",
        "                 train_loader=train_loader,\n",
        "                 valid_loader=valid_loader           \n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDB3tltpCWlT"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
        "\n",
        "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPp6gyfUCWlT"
      },
      "outputs": [],
      "source": [
        "#TODO: Declare your HP ranges, metrics etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "DqZfNkFnCWlT",
        "outputId": "069d7e6e-794e-46ed-91b9-f10bb5077da7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-55b822c2ede9>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    estimator = # TODO: Your estimator here\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#TODO: Create estimators for your HPs\n",
        "\n",
        "estimator = # TODO: Your estimator here\n",
        "\n",
        "tuner = # TODO: Your HP tuner here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_mVT_8ICWlT"
      },
      "outputs": [],
      "source": [
        "# TODO: Fit your HP Tuner\n",
        "tuner.fit() # TODO: Remember to include your data channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "eYc_2Ey8CWlT",
        "outputId": "d8261dc7-8e45-4273-8b3e-56b1dee6ed47"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-5fea354ec23d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    best_estimator = #TODO\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# TODO: Get the best estimators and the best HPs\n",
        "\n",
        "best_estimator = #TODO\n",
        "\n",
        "#Get the hyperparameters of the best trained model\n",
        "best_estimator.hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hk-Pdr1CWlU"
      },
      "source": [
        "## Model Profiling and Debugging\n",
        "TODO: Using the best hyperparameters, create and finetune a new model\n",
        "\n",
        "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZzasIZC3CWlU"
      },
      "outputs": [],
      "source": [
        "# TODO: Set up debugging and profiling rules and hooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "v0c8DQv3CWlU",
        "outputId": "8bdbc567-cad6-44fa-c6a3-f1ee8c2e3b9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-01210f8774ec>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    estimator = # TODO: Your estimator here\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create and fit an estimator\n",
        "\n",
        "estimator = # TODO: Your estimator here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IVpxgvy8CWlU"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot a debugging output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUPSkYnUCWlU"
      },
      "source": [
        "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
        "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_8ODM8ACWlU"
      },
      "outputs": [],
      "source": [
        "# TODO: Display the profiler output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSVA5s8oCWlU"
      },
      "source": [
        "## Model Deploying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuA9fNalCWlU"
      },
      "outputs": [],
      "source": [
        "# TODO: Deploy your model to an endpoint\n",
        "\n",
        "predictor=estimator.deploy() # TODO: Add your deployment configuration like instance type and number of instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJw9NoGdCWlU"
      },
      "outputs": [],
      "source": [
        "# TODO: Run an prediction on the endpoint\n",
        "\n",
        "image = # TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
        "response = predictor.predict(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeEEYE7eCWlV"
      },
      "outputs": [],
      "source": [
        "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
        "predictor.delete_endpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEvqJJeACWlV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.g4dn.xlarge",
    "interpreter": {
      "hash": "ba27402638e91eb0fdee4a7d4d07899a3c7f7c55c127b9076d00f2ca1d4554f7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "train_and_deploy (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}